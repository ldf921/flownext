optimizer:
  learning_rate:
    - [100000.0, 5.0e-4]
    - [300000.0, 1.0e-4]
    - [400000.0, 5.0e-5]
  params:
    wd:
      2.0e-4
  lr_mult:
    1
  lr_mult_layer:
    features:
      0.2 # Learning rate of 1e-4
  type:
    adam

network:
  class:
    HybridNetFP
  scale:
    20.0

loss:
  scales:
    - 32 
    - 16
    - 8
    - 4
  weights:
    - 0.1
    - 0.1
    - 0.2
    - 0.4
  match:
    upsampling